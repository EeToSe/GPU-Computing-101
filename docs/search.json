[
  {
    "objectID": "src/gpu.html",
    "href": "src/gpu.html",
    "title": "How a GPU works",
    "section": "",
    "text": "How many calculations do graphics cards perform:\nIntuition: To match 36T calc/s, imagine ~4,400 Earths where every person completes 1 calculation each second.",
    "crumbs": [
      "Architecture",
      "How a GPU works"
    ]
  },
  {
    "objectID": "src/gpu.html#gpu-vs-cpu",
    "href": "src/gpu.html#gpu-vs-cpu",
    "title": "How a GPU works",
    "section": "GPU vs CPU",
    "text": "GPU vs CPU\n\nGPU: “&gt;10,000 cores,” optimized for massively parallel, simple arithmetic; cannot run OS or directly interface with input devices or networks.\nCPU: “~24 cores,” higher per-core speed; flexible instruction support; runs OS and diverse I/O.\nAnalogy: GPU = cargo ship (huge throughput, slower per unit). CPU = jumbo jet (lower throughput, faster per unit, flexible).",
    "crumbs": [
      "Architecture",
      "How a GPU works"
    ]
  },
  {
    "objectID": "src/gpu.html#gpu-hierarchy-nvidia-ga102",
    "href": "src/gpu.html#gpu-hierarchy-nvidia-ga102",
    "title": "How a GPU works",
    "section": "GPU Hierarchy (NVIDIA GA102)",
    "text": "GPU Hierarchy (NVIDIA GA102)\n\n\n\n\n\n\n\n\n\n\nLevel\nComponent\nCount per Parent Component\nTotal Count (on GPU)\nNotes\n\n\n\n\n1. Chip\nFull GA102 GPU\nN/A\n1\nContains 28.3 billion transistors.\n\n\n2. Cluster\nGraphics Processing Cluster (GPC)\n7 GPCs per GPU\n7\nThe highest-level logical block.\n\n\n3. Processor\nStreaming Multiprocessor (SM)\n12 SMs per GPC\n84\nThe main processing engine of the GPU.\n\n\n4. Scheduler\nWarp Scheduler / Processing Block\n4 Schedulers per SM\n336\nManages and dispatches warps (groups of 32 threads).\n\n\n5. Core\nCUDA Core\n32 CUDA Cores per Scheduler\n10,752\nExecutes floating-point and integer math.\n\n\n\nTensor Core\n1 Tensor Core per Scheduler\n336\nAccelerates AI matrix calculations.\n\n\n\nRay Tracing (RT) Core\n1 RT Core per SM\n84\nAccelerates ray-triangle intersection tests.",
    "crumbs": [
      "Architecture",
      "How a GPU works"
    ]
  },
  {
    "objectID": "src/gpu.html#one-chip-multiple-products-binning",
    "href": "src/gpu.html#one-chip-multiple-products-binning",
    "title": "How a GPU works",
    "section": "One Chip, Multiple Products (Binning)",
    "text": "One Chip, Multiple Products (Binning)\nDuring the manufacturing process, sometimes patterning errors, dust particles, or other manufacturing issues cause damage and create defective areas of the circuit. Instead of throwing out the entire chip because of a small defect, engineers find the defective region and permanently isolate and deactivate the nearby circuitry. By having a GPU with a highly repetitive design, a small defect in one core only damages that particular streaming multiprocessor circuit and doesn’t affect the other areas of the chip. As a result, these chips are tested and categorized, or binned, according to the number of defects.\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nCUDA Cores\nActive SMs\nSMs Disabled\nBoost Clock (MHz)\nMemory\nRelease Date\n\n\n\n\nRTX 3080\n8,704\n68\n16\n1710\n10 GB\nSeptember 17, 2020\n\n\nRTX 3080 Ti\n10,240\n80\n4\n1665\n12 GB\nJune 3, 2021\n\n\nRTX 3090\n10,496\n82\n2\n1695\n24 GB\nSeptember 24, 2020\n\n\nRTX 3090 Ti\n10,752\n84\n0\n1860\n24 GB\nMarch 29, 2022\n\n\n\n\nSame GA102 used across RTX 3080, 3080 Ti, 3090, 3090 Ti.\nYield/defect handling: isolate defective regions; disable affected SMs; bin by working units.\nAlso differ by: max clock, VRAM quantity/generation.",
    "crumbs": [
      "Architecture",
      "How a GPU works"
    ]
  },
  {
    "objectID": "src/gpu.html#inside-a-cuda-core-fp32-alu",
    "href": "src/gpu.html#inside-a-cuda-core-fp32-alu",
    "title": "How a GPU works",
    "section": "Inside a CUDA Core (FP32 ALU)",
    "text": "Inside a CUDA Core (FP32 ALU)\n\nA CUDA core = 1 FP32 ALU capable of one FMA (A×B + C) per clock (2 FLOPs).\nEach SM in GA102 (Ampere) has\n\n128 FP32 cores.\n2 FP64 ALUs (double-precision), far fewer than FP32 — ~1/64 FP32 throughput.\nINT32 ALUs — in Ampere, half of FP32 cores are hybrid FP32/INT32 units, which can execute either FP32 or INT32 per cycle.\n4 Tensor Cores (3rd gen) for matrix multiply-accumulate at lower precisions (FP16, BF16, TF32, INT8, INT4).\n1 RT Core for ray tracing.\nSpecial Function Units (SFUs), Load/Store units, warp schedulers.\n\nMost common method for theoretical FP32 peak:\n\\[\n\\text{Peak FLOPS} = (\\text{\\#FP32 cores}) \\times (\\text{clock freq}) \\times 2\n\\] For RTX 3090: \\(10{,}496 \\times 1.70\\,\\text{GHz} \\times 2 \\approx 35.7\\) TFLOPS FP32.\nActual achieved FLOPS depend heavily on instruction mix, memory bandwidth, and pipeline utilization — real workloads often achieve a fraction of the peak.",
    "crumbs": [
      "Architecture",
      "How a GPU works"
    ]
  },
  {
    "objectID": "src/gpu.html#graphic-card-components",
    "href": "src/gpu.html#graphic-card-components",
    "title": "How a GPU works",
    "section": "Graphic card components",
    "text": "Graphic card components\nOn-die (GA102)\n\nMemory controllers: 12 × 32-bit GDDR6X controllers (384-bit total), positioned around die edges for shortest routing.\nNVLink controllers: Present only on RTX 3090 and pro SKUs, absent on some consumer GA102 cards.\nPCIe interface: PCIe 4.0 controller and PHY integrated on die.\nL2 cache: 6 MB unified SRAM, distributed near memory controllers.\nGigaThread Engine: Global scheduler managing all GPCs (Graphics Processing Clusters) and SMs.\n\nOn the graphics card PCB\n\nDisplay outputs: Typically 3× DisplayPort 1.4a + 1× HDMI 2.1 (FE layout; AIB may vary).\nPower input: 12-pin NVIDIA connector (FE) or 2–3× 8-pin PCIe connectors; delivers +12 V.\nPCIe edge connector: ×16, Gen 4.0; provides power (max 75 W) and data lanes.\nVoltage Regulator Module (VRM): Steps 12 V down to ~0.7–1.1 V for GPU core, ~1.35 V for GDDR6X; capable of supplying hundreds of watts (300–400 A).",
    "crumbs": [
      "Architecture",
      "How a GPU works"
    ]
  },
  {
    "objectID": "src/gpu.html#graphics-memory-vram",
    "href": "src/gpu.html#graphics-memory-vram",
    "title": "How a GPU works",
    "section": "Graphics Memory (VRAM)",
    "text": "Graphics Memory (VRAM)\n\nThe graphics card has 24 gigabytes of GDDR6X SDRAM (graphics memory).\nThe GPU contains a very small 6-megabyte Level 2 cache for immediate data access.\nDuring loading screens, 3D models are moved from the solid-state drive (SSD) into the graphics memory.\nTo render a game, data is continuously transferred between the graphics memory and the GPU’s cache.\nThe graphics memory has a 384-bit bus width, which is the amount of data it can transfer simultaneously.\nThis results in a total bandwidth of approximately 1.15 terabytes per second (TB/s).\nFor comparison, the CPU’s standard memory (DRAM) has a much smaller 64-bit bus width and a bandwidth of only ~64 gigabytes per second (GB/s).\n\nSignaling/encoding:\n\n\n\n\n\n\n\n\n\nStep\nCalculation\nResult\nNotes\n\n\n\n\nCK (Base)\nGiven\n1219 MHz\nCommand clock.\n\n\nWCK (High-Speed Mode)\n4 × CK\n4876 MHz\nQDR mode ratio for peak performance.\n\n\nSymbol Rate\n2 × WCK (DDR)\n9752 Msymbols/s\nTransfers on both clock edges; equivalent to 8 × CK.\n\n\nEffective Data Rate (with PAM-4)\nSymbol Rate × 2\n19.5 Gbps\nGDDR6X-specific; bits per symbol doubles bandwidth.\n\n\nBus Width\nGiven\n384 bits\nNumber of data pins in the memory interface.\n\n\nTotal Throughput\nEffective Data Rate × Bus Width\n7488 Gbits/s\nAggregate bit rate across all pins (19.5 Gbps/pin × 384 pins).\n\n\nMemory Bandwidth\nTotal Throughput / 8\n936 GB/s\nConverted from bits to bytes (divide by 8 bits/byte) for standard bandwidth metric.\n\n\n\n\nGDDR6X: PAM-4 (2 bits per symbol via 4 voltage levels).\nGDDR7: PAM-3 (ternary digits, voltages −1/0/+1).\n\nExample from talk: 3 binary bits → 2 ternary digits; combined with an 11-bit → 7 ternary-digit scheme → send 276 binary bits using 176 ternary digits.\n\nMotivation for PAM-3: reduce encoder complexity, improve SNR, improve power efficiency.\n\nHigh-Bandwidth Memory (HBM) for AI\n\nStructure: stacked DRAM with TSVs (through-silicon vias) → “memory cube” around AI chips.\nHBM3E capacities: 24–36 GB per cube; up to ~192 GB around one AI chip.\nClaim: ~30% less power than “competitive products”.\nSystem context: AI accelerator systems $25–40k; often backordered.",
    "crumbs": [
      "Architecture",
      "How a GPU works"
    ]
  },
  {
    "objectID": "src/gpu.html#embarrassingly-parallel",
    "href": "src/gpu.html#embarrassingly-parallel",
    "title": "How a GPU works",
    "section": "“Embarrassingly Parallel”",
    "text": "“Embarrassingly Parallel”\n\nDefinition: problems that need little/no effort to split into parallel tasks.\nGPU fit: SIMD/SIMT executing the same instructions over many data elements.\nExamples: video game rendering, Bitcoin mining, neural networks/AI.\n\nVertex Translation (Model → World)\n\nObject example: cowboy hat with ~28k triangles, ~14k vertices (x,y,z); origin at (0,0,0) in model space.\nWorld assembly: place hundreds of objects into world space; translate each vertex by object’s world origin (and similarly handle rotation/scale in additional steps).\nScene numbers (from talk): 5,629 objects, 8.3 million vertices → ~25 million additions for translation step.\nKey idea: each vertex transform is independent → perfect for massive parallelism.",
    "crumbs": [
      "Architecture",
      "How a GPU works"
    ]
  },
  {
    "objectID": "src/gpu.html#mapping-computation-to-hardware-simd-simt",
    "href": "src/gpu.html#mapping-computation-to-hardware-simd-simt",
    "title": "How a GPU works",
    "section": "Mapping Computation to Hardware (SIMD → SIMT)",
    "text": "Mapping Computation to Hardware (SIMD → SIMT)\n\nThread: executes one instruction on one data element (conceptually maps to a CUDA core).\nWarp: 32 threads executing the same instruction sequence.\nThread Block: a group of warps handled by an SM; can use shared L1 cache (128 KB per SM, as stated) for data sharing.\nGrid: many blocks across the entire GPU.\nGlobal scheduler: Gigathread Engine maps blocks to SMs.\nSIMT vs SIMD (evolution):\n\nOlder GPUs: SIMD lockstep within warps.\nNewer GPUs: SIMT — same instruction stream, but threads can progress at different rates (each has its own program counter).\n\nWarp divergence: data-dependent branches serialize paths; reconvergence at synchronization barriers.",
    "crumbs": [
      "Architecture",
      "How a GPU works"
    ]
  },
  {
    "objectID": "src/gpu.html#tensor-cores-matrix-engines",
    "href": "src/gpu.html#tensor-cores-matrix-engines",
    "title": "How a GPU works",
    "section": "Tensor Cores — Matrix Engines",
    "text": "Tensor Cores — Matrix Engines\n\nOperation form: D = A×B + C (matrix multiply-accumulate) with all three inputs “ready,” executing concurrently across tiles.\nUse cases: neural networks/AI, geometric transformations.\nNote: Ray Tracing Cores covered in a separate video.\n\nlinks:\n\nHow do Graphics Cards Work? Exploring GPU Architecture\nHow GPU Computing Works | GTC 2021",
    "crumbs": [
      "Architecture",
      "How a GPU works"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This is a Quarto website for learning the gpu.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "src/cpu.html",
    "href": "src/cpu.html",
    "title": "How a CPU works",
    "section": "",
    "text": "Before any computation can happen, we need a stage and actors. In computing, these are the CPU and RAM.\n\nThe Central Processing Unit (CPU): This is the “brain” of the computer. It’s where all the thinking and calculation happens. It contains several key sub-components:\n\nControl Unit (CU): The “brain” or “foreman” of the operation. It fetches and decodes instructions and sends out command signals to the other components.\nArithmetic Logic Unit (ALU): The dedicated calculator for all math and logic.\nRegisters: A small number of extremely fast memory locations on the CPU chip itself, acting as an immediate workbench.\n\nRandom Access Memory (RAM): This is the CPU’s primary workspace. It’s a vast grid of temporary storage where all the necessary information for active programs is held. The CPU constantly communicates with RAM to get what it needs.\nBuses: The electronic highways that connect all the components, allowing data and instructions to move between them.",
    "crumbs": [
      "Architecture",
      "How a CPU works"
    ]
  },
  {
    "objectID": "src/cpu.html#part-1-the-stage-and-the-actors---the-core-hardware",
    "href": "src/cpu.html#part-1-the-stage-and-the-actors---the-core-hardware",
    "title": "How a CPU works",
    "section": "",
    "text": "Before any computation can happen, we need a stage and actors. In computing, these are the CPU and RAM.\n\nThe Central Processing Unit (CPU): This is the “brain” of the computer. It’s where all the thinking and calculation happens. It contains several key sub-components:\n\nControl Unit (CU): The “brain” or “foreman” of the operation. It fetches and decodes instructions and sends out command signals to the other components.\nArithmetic Logic Unit (ALU): The dedicated calculator for all math and logic.\nRegisters: A small number of extremely fast memory locations on the CPU chip itself, acting as an immediate workbench.\n\nRandom Access Memory (RAM): This is the CPU’s primary workspace. It’s a vast grid of temporary storage where all the necessary information for active programs is held. The CPU constantly communicates with RAM to get what it needs.\nBuses: The electronic highways that connect all the components, allowing data and instructions to move between them.",
    "crumbs": [
      "Architecture",
      "How a CPU works"
    ]
  },
  {
    "objectID": "src/cpu.html#part-2-the-script-and-props---what-resides-in-ram",
    "href": "src/cpu.html#part-2-the-script-and-props---what-resides-in-ram",
    "title": "How a CPU works",
    "section": "Part 2: The Script and Props - What Resides in RAM",
    "text": "Part 2: The Script and Props - What Resides in RAM\nNow that we have the workspace (RAM), we must define what’s inside it. RAM holds a mixture of three types of information, all stored as raw binary data (1s and 0s). To the RAM, it’s all the same, but to the CPU, they have very different meanings:\n\nInstructions: These are the commands that form a computer program. They are the “script” that tells the CPU what to do.\n\n\nOpcode (Operation Code): The “verb” of the command. It’s a binary pattern that specifies the action to perform (e.g., ADD, LOAD, STORE).\nOperand(s): The “nouns” of the command. They specify the data or the location of the data to be used, such as a register number or a memory address.\n\n\nData: This is the raw material the instructions work on—the numbers, text, and values that are processed.\nAddresses: These are special numbers that act as pointers, indicating the specific location of other instructions or pieces of data within RAM’s grid.",
    "crumbs": [
      "Architecture",
      "How a CPU works"
    ]
  },
  {
    "objectID": "src/cpu.html#part-3-the-heartbeat---the-system-clock",
    "href": "src/cpu.html#part-3-the-heartbeat---the-system-clock",
    "title": "How a CPU works",
    "section": "Part 3: The Heartbeat - The System Clock",
    "text": "Part 3: The Heartbeat - The System Clock\nAt the absolute core of the CPU is the Clock. It’s a crystal oscillator that produces a steady, rhythmic pulse billions of times per second. This pulse, called a clock cycle, is the fundamental unit of time for the CPU. Its speed is measured in Hertz (Hz), with modern CPUs operating in the Gigahertz (GHz), or billions of cycles per second. The clock’s purpose is to synchronize all the CPU’s components, ensuring every action happens in a coordinated and orderly fashion.",
    "crumbs": [
      "Architecture",
      "How a CPU works"
    ]
  },
  {
    "objectID": "src/cpu.html#part-4-the-core-task---the-instruction-cycle",
    "href": "src/cpu.html#part-4-the-core-task---the-instruction-cycle",
    "title": "How a CPU works",
    "section": "Part 4: The Core Task - The Instruction Cycle",
    "text": "Part 4: The Core Task - The Instruction Cycle\nTo understand how a CPU works, we must first look at what it can do — not just how it does it. That “what” is called the Instruction Set Architecture (ISA) — a list of all the basic commands the CPU understands. Think of it as the CPU’s vocabulary.\n\nData Transfer Instructions: Moving data around\n\nThese instructions move data between memory, the CPU, and devices.\n\nLOAD: Move data from memory → into CPU register\nSTORE: Move data from CPU register → into memory\nIN / OUT: Communicate with external devices (e.g., keyboard, screen)\n\nAnalogy: Like fetching ingredients from the fridge (LOAD) and putting the cooked meal back (STORE).\n\nArithmetic and Logic Instructions: Doing math and comparisons\n\nHandled by the CPU’s ALU (Arithmetic Logic Unit).\n\nADD, SUBTRACT, MULTIPLY: Perform basic math\nCOMPARE: Compare two values and set internal flags (e.g., “are they equal?” → set the Zero flag)\n\nFlags help the CPU remember the result of comparisons for later use (e.g., in decision-making).\n\nControl Flow Instructions: Making decisions\n\nThese control the sequence of execution — whether to continue normally or jump elsewhere.\n\nJUMP: Go to another instruction (unconditionally)\nJUMP IF: Jump only if a condition is true (e.g., Jump if Equal = only if the COMPARE said they were equal)\n\nThe CPU executes programs by repeating a fundamental loop called the Instruction Cycle, driven by the clock. Here is the correct, logical sequence of events:\n\nBegin Fetch: The Control Unit consults a special register called the Instruction Pointer (IP) or Program Counter (PC). This pointer’s sole job is to hold the memory address of the next instruction to execute.\nFetch from RAM: The CU retrieves the binary instruction from the memory address indicated by the IP.\nIncrement the Pointer: Immediately after the fetch, the IP’s value is increased to point to the next instruction’s address, ensuring the program will proceed sequentially.\nDecode: The CU decodes the fetched instruction, understanding what action needs to be performed and what data is involved.\nExecute: The CU sends command signals to the ALU and Registers. The ALU performs the actual calculation or logical comparison.\nStore: The result of the execution is saved, typically back into a register for quick reuse.\n\nA single instruction can take multiple clock cycles to complete. The efficiency of a CPU is therefore not just about clock speed (GHz), but also about how many Instructions Per Clock (IPC) it can achieve.\nrelevant links:\n\nHow a CPU Works - Youtube\nVisual Transistor-level Simulation of the 6502 CPU\nPutting the “You” in CPU",
    "crumbs": [
      "Architecture",
      "How a CPU works"
    ]
  }
]